\relax 
\@writefile{toc}{\contentsline {section}{\numberline {1}Part A: Computing Policies}{1}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Formulation of the Taxi Domain as an MDP:}{1}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Implementing Value Iteration for the Taxi Domain:}{2}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Max Norm distance vs iteration Index}}{3}{}\protected@file@percent }
\newlabel{fig:Max Norm distance vs iteration Index}{{1}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3}Implementing Policy Iteration for the Taxi Domain:}{4}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Policy Loss}}{4}{}\protected@file@percent }
\newlabel{fig:Policy Loss}{{2}{4}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Part B: Incorporating Learning}{5}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Implementing the approaches to learn the optimal policy:}{5}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Number of iterations VS Discounted reward:}{5}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces QLearning e greedy}}{6}{}\protected@file@percent }
\newlabel{fig:QLearning_e-greedy}{{3}{6}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces QLearning exploration decay}}{6}{}\protected@file@percent }
\newlabel{fig:QLearning_exploration-decay}{{4}{6}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces SARSA e greedy}}{7}{}\protected@file@percent }
\newlabel{fig:SARSA_e-greedy}{{5}{7}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces SARSA exploration decay}}{7}{}\protected@file@percent }
\newlabel{fig:SARSA_exploration-decay}{{6}{7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Execution using learned policy:}{8}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Analysing effect on policy by changing epsilon and alpha:}{8}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces alpha = 0.1 and varying epsilon}}{8}{}\protected@file@percent }
\newlabel{fig: alpha = 0.1 and varying epsilon}{{7}{8}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces epsilon = 0.1 and varying alpha}}{9}{}\protected@file@percent }
\newlabel{fig: epsilon = 0.1 and varying alpha}{{8}{9}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5}Testing on larger grid:}{9}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Instance having shortest path length = 36 (best reward sum = -17.8)}}{10}{}\protected@file@percent }
\newlabel{fig: Instance having shortest path length = 36 }{{9}{10}}
\gdef \@abspage@last{10}
