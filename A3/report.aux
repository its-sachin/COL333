\relax 
\@writefile{toc}{\contentsline {section}{\numberline {1}Part A: Computing Policies}{1}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Formulation of the Taxi Domain as an MDP:}{1}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Implementing Value Iteration for the Taxi Domain:}{2}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Epsilon vs iteration Index}}{3}{}\protected@file@percent }
\newlabel{fig:Epsilon vs iteration Index}{{1}{3}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Max Norm distance vs iteration Index}}{3}{}\protected@file@percent }
\newlabel{fig:Max Norm distance vs iteration Index}{{2}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3}Implementing Policy Iteration for the Taxi Domain:}{4}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Policy Loss}}{5}{}\protected@file@percent }
\newlabel{fig:Policy Loss}{{3}{5}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Part B: Incorporating Learning}{6}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Implementing the approaches to learn the optimal policy:}{6}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Number of iterations VS Discounted reward:}{6}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces No of episodes VS Discounted Rewards for different Algo}}{7}{}\protected@file@percent }
\newlabel{fig:No of episodes VS Discounted Rewards for different Algo}{{4}{7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Execution using learned policy:}{8}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Analysing effect on policy by changing $\epsilon $ and $\alpha $:}{8}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces alpha = 0.1 and varying epsilon}}{8}{}\protected@file@percent }
\newlabel{fig: alpha = 0.1 and varying epsilon}{{5}{8}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces epsilon = 0.1 and varying alpha}}{9}{}\protected@file@percent }
\newlabel{fig: epsilon = 0.1 and varying alpha}{{6}{9}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5}Testing on larger grid:}{10}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Instance having shortest path length = 36 (best reward sum = -17.8)}}{10}{}\protected@file@percent }
\newlabel{fig: Instance having shortest path length = 36 }{{7}{10}}
\gdef \@abspage@last{11}
